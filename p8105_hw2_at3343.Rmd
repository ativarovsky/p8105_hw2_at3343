---
title: "Homework 2"
author: "Alice Tivarovsky"
date: "9/26/2019"
output: github_document
editor_options: 
  chunk_output_type: inline
---
# Problem 1

## Mr Trash Wheel Data

We are reading in the Mr. Trash Wheel dataset, stored as an excel file. We will remove the last column, which contains notes. 

```{r data_frame1}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)

# Importing Mr Trash Data
mr_trash_data = read_excel("../data/mr_trash_dataset.xlsx")
mr_trash_data = janitor::clean_names(mr_trash_data)

# Removing last column of comments
mr_trash_data = select(mr_trash_data, -x15, -x16, -x17)
mr_trash_data
```

The "raw" dataframe contains 406 entries and 14 columns. 

We will now remove any rows that have dumpster = NA and convert the column "sports_balls" to an integer vector rounded to the nearest whole number. 

```{r data_frame2}

# Removing rows with "NA" for dumpster
mr_trash_data = drop_na(mr_trash_data, dumpster)

# Rounding sports_balls to nearest integer
mr_trash_data = mutate(mr_trash_data, sports_balls = round(as.integer(sports_balls, digits = 0)))
mr_trash_data
```

## Precipitatin Data

We will now import and clean 2017, 2018, and 2019 precipitation data from the Mr. Trash Wheel excel file.  

```{r data_frame3}
# Importing 2017, 2018, and 2019 precipitation data. Skipping first row

precip_data_19 = 
  read_excel("../data/mr_trash_dataset.xlsx",4, skip = 1) %>% 
  janitor::clean_names()

precip_data_18 = 
  read_excel("../data/mr_trash_dataset.xlsx",5, skip = 1) %>% 
  janitor::clean_names()

precip_data_17 = 
  read_excel("../data/mr_trash_dataset.xlsx",6, skip = 1) %>% 
  janitor::clean_names()

```


We note that the 2017 and 2018 datasets are complete, but the 2019 dataset is missing values for July - December. We also note that all three datasets contain a row at the end with total precipitation for the year, which we don't need. We will omit these rows, along with the missing values in 2019, and add a year variable before combining the datasets. 

```{r data_frame4}

# Removing rows with "NA" for precipation or month (i.e. the totals row) 
precip_data_19 = drop_na(precip_data_19)
precip_data_17 = drop_na(precip_data_17)
precip_data_18 = drop_na(precip_data_18)

# Renaming precipitation columns with year
precip_data_17 = rename(precip_data_17, precip_2017 = total)
precip_data_18 = rename(precip_data_18, precip_2018 = total)
precip_data_19 = rename(precip_data_19, precip_2019 = total)

precip_data_17
precip_data_18
precip_data_19

```

Next, we will combine the precipitation data for 2017 and 2018, and combine 2019 into the resulting dataset using a left-join since 2019 contains less data than 2017 and 2018. 

```{r data_frame5}

precip_data =  
  left_join(precip_data_17, precip_data_18, by = "month") %>% 
  left_join(precip_data_19, by = "month")

# Converting month to character variable
precip_data =
  mutate(precip_data, month = as.numeric(month),
         month = month.name[month]) 

precip_data
```

## Conclusion

The Mr.Trash Wheel dataset catalogs the weight, volume and categories of trash processed by every Mr. Trash Wheel dumpster, organized by year and month. It contains `r count(mr_trash_data)` observations. The precipitation dataset catalogs precipitation in inches by month for 2017 - 2019. It contains `r count(precip_data)` observations. The total precipitation for 2018 was `r sum(pull(precip_data, precip_2018))` inches. The median number of sports balls in a dumpster in 2017 was 
`r median(pull(mr_trash_data, sports_balls))`. 

# Problem 2

## Pols Data

First, we read and clean the pols-month dataset. 

```{r data_frame6}
pols_month = 
  read_csv(file = "../data/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, c("year", "month", "day"), "-") %>% 
  mutate(month = as.numeric(month),
         month = month.abb[month]) %>% 
  mutate(democrat_president = prez_dem == 1) %>% 
  select(-prez_dem, -prez_gop, -day) 

pols_month
``` 
Pols_month contains 822 rows and 9 columns. 

Next, we read and clean the snp dataset. 

```{r data_frame7}
snp_data = 
  read_csv(file = "../data/snp.csv") %>% 
  janitor::clean_names()  %>% 
  separate(date, c("month", "day", "year"), "/") %>% 
  select(year, month, day, everything()) %>% 
  mutate(month = as.numeric(month),
         month = month.abb[month]) %>%
  arrange(year, month)

snp_data
```
The snp_data dataset contains 787 rows. 

Next, we read and clean the unemployment dataset. This entails switching the format from wide to long 

```{r data_frame8}
unemp_data = 
  read_csv(file = "../data/unemployment.csv") %>% 
  pivot_longer(Jan:Dec, 
             names_to = "month",
             values_to = "unemployment") %>% 
   rename(year = Year) %>% 
   mutate(year = as.character(year))

unemp_data
```
The unemp_data dataset contains 816 observations. 

Finally, we combine the three datasets, starting by combining pols and snp, then joining the combined dataset with the unemployment dataset. 

```{r data_frame9}
pols_snp = 
  right_join(pols_month, snp_data, by = c("year", "month")) %>% 
  left_join(unemp_data, by = c("year", "month"))

pols_snp
```

## Conclusion

The pols_month dataset contains 822 observations and 9 variables. It catalogs congressional representation by party, including senators, representatives, governors, and the president. The snp_data dataset contains 787 rows and provids the closing value of the S&P 500 on the specified date. The unemployment dataset provides the percentage unemployed by year and month and contains 816 observations and 3 variables. 

The dataset obtained after merging the three dataset contains 15 variables and 787 rows spanning 1950 - 2015. We noted that there was no data for S&P 500 from January 1947 through December 1949, most likely because the index did not exist (although, per Wikipedia, the S&P index did not exist until 1957...) and used a right_join for this reason. The key variables are the logical variable indicating whether the president was a democrat or republican, unemployment percentage, and S&P500 closing value. 

# Problem 3

Reading and tidying baby names dataset. 

```{r data_frame10}
pop_names = 
  read_csv(file = "../data/Popular_Baby_Names.csv") %>% 
  janitor::clean_names() %>% 
  # The resulting data-frame contains 19,148 rows
  mutate(ethnicity = replace(ethnicity, ethnicity == "ASIAN AND PACI", "ASIAN AND PACIFIC ISLANDER"), ethnicity = replace(ethnicity, ethnicity == "BLACK NON HISP", "BLACK NON HISPANIC"), ethnicity = replace(ethnicity, ethnicity == "WHITE NON HISP", "WHITE NON HISPANIC")) %>% 
  mutate(childs_first_name = toupper(childs_first_name)) %>% 
  distinct()
pop_names
```


Now we are making a table showing the rank in popularity of the name “Olivia” as a female baby name over time. First we create a dataset from pop_names with only the information we need. 

```{r data_frame11}
olivia_pop = filter(pop_names, childs_first_name == "OLIVIA") %>% 
select(ethnicity, year_of_birth, rank) %>% 
  pivot_wider(names_from = "year_of_birth",values_from = "rank")

olivia_pop
```
Creating a table for olivia_pop rank over time. 

```{r data_frame12}
library(knitr)
olivia_pop = rename(olivia_pop, "Ethnicity" = "ethnicity")
kable(olivia_pop)
```

Now we create a table for the most popular boys' name over time and make a table using kable() as above. 

```{r}
boys_name = 
  filter(pop_names, gender == "MALE", rank == 1) %>% 
  select(ethnicity, year_of_birth, childs_first_name) %>% 
  rename("Ethnicity" = "ethnicity") %>% 
  pivot_wider(names_from = "year_of_birth",values_from = "childs_first_name")
boys_name

kable(boys_name)
```

Finally, we create a scatterplot of male, white non-hispanic children born in 2016, plot showing the number of children with a name (y axis) against the rank in popularity of that name (x axis).

```{r}
white_boys_2016 = 
  filter(pop_names, gender == "MALE", year_of_birth == 2016, ethnicity == "WHITE NON HISPANIC")

white_boys_2016

plot_1 = ggplot(white_boys_2016, aes(x = rank, y = count)) + geom_point()
plot_1
```



